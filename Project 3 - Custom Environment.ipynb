{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# stuff from gym\n",
    "import gym \n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "\n",
    "#helpers\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "#stuff from stable baselines\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Discrete(3).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09067822, 0.23819503, 0.84387136],\n",
       "       [0.4592315 , 0.20830125, 0.2523304 ],\n",
       "       [0.93719894, 0.21703143, 0.25233933]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Box(0,1, shape=(3,3)).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " array([[0.3397607 , 0.03724467, 0.06978406],\n",
       "        [0.43154442, 0.8433508 , 0.89047474],\n",
       "        [0.93551767, 0.0895842 , 0.2892942 ]], dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuple((Discrete(3),Box(0,1, shape=(3,3)))).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('height', 0), ('speed', array([71.91626], dtype=float32))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dict({'height':Discrete(2), 'speed':Box(0,100, shape=(1,))}).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1], dtype=int8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiBinary(4).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiDiscrete([5,2,2]).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building an Environment\n",
    "- Build an agent to give us the best shower possible\n",
    "- Random temperature\n",
    "- 37 to 39 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(3) #tap up, tap down or tap unchanged\n",
    "        #temp high and low\n",
    "        self.observation_space = Box(low= np.array([0]), high=np.array([100])) \n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        self.shower_length = 60\n",
    "        \n",
    "    def step(self, action):\n",
    "        #apply temp adj\n",
    "        self.state += action-1\n",
    "        \n",
    "        #Decrease shower time\n",
    "        self.shower_length -= 1\n",
    "        \n",
    "       #Calculate Reward\n",
    "        if self.state >=37 and self.state <=39:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        if self.shower_length <=0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "        \n",
    "    def render(self):\n",
    "        #implement visualization\n",
    "        pass\n",
    "    def reset(self):\n",
    "        self.state = np.array([38+ random.randint(-3,3)]).astype(float)\n",
    "        self.shower_length= 60\n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ShowerEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1, Score:-36\n",
      "Episode:2, Score:-8\n",
      "Episode:3, Score:2\n",
      "Episode:4, Score:-36\n",
      "Episode:5, Score:-26\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes + 1):\n",
    "  obs = env.reset()\n",
    "  done = False\n",
    "  score = 0\n",
    "\n",
    "  while not done:\n",
    "   # env.render()\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "  print('Episode:{}, Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join('Training','Logs')\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log= log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_7\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60       |\n",
      "|    ep_rew_mean     | -12.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 2041     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -8.76        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1313         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035967003 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.697       |\n",
      "|    explained_variance   | 0.000111     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.6         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 95.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -8.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1183         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038415156 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.735       |\n",
      "|    explained_variance   | -0.000232    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 55.9         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -10.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1103         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040302374 |\n",
      "|    clip_fraction        | 0.0536       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.77        |\n",
      "|    explained_variance   | 0.000804     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.2         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | 0.00106      |\n",
      "|    value_loss           | 78.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -11.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1087         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030530826 |\n",
      "|    clip_fraction        | 0.0369       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.759       |\n",
      "|    explained_variance   | 0.00135      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.5         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | 0.00106      |\n",
      "|    value_loss           | 72.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -8.58        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1075         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041051377 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.771       |\n",
      "|    explained_variance   | 0.00501      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.2         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    value_loss           | 89.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -9.16        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1064         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007808678 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.762       |\n",
      "|    explained_variance   | -0.00402     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.8         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | 0.00116      |\n",
      "|    value_loss           | 80           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -8.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1055         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025216253 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.79        |\n",
      "|    explained_variance   | 0.0079       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.9         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 76.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -13.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1052        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013742054 |\n",
      "|    clip_fraction        | 0.0719      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.738      |\n",
      "|    explained_variance   | -0.0248     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.9        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | 0.000308    |\n",
      "|    value_loss           | 92.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -9.94        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1044         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045469073 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.715       |\n",
      "|    explained_variance   | -0.00075     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.1         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    value_loss           | 73.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1f9a0058790>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Savel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_path = os.path.join('Training','Saved Models', 'Shower_Model_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(shower_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-24.0, 54.99090833947008)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
